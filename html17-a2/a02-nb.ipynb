{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "from mnist import MNIST # run from Anaconda shell: pip install python-mnist\n",
    "from collections import Counter\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "inNotebook = False # change this to True if you use a notebook\n",
    "def nextplot():\n",
    "    if inNotebook:\n",
    "        plt.figure()  # this creates a new plot\n",
    "    else:\n",
    "        plt.clf()     # and this clears the current one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mndata = MNIST(\"data/\")\n",
    "X, y = mndata.load_training()\n",
    "y = np.array(y, dtype=\"uint8\")\n",
    "X = np.array([np.array(x) for x in X], dtype=\"uint8\")\n",
    "N, D = X.shape\n",
    "Xtest, ytest = mndata.load_testing()\n",
    "ytest = np.array(ytest, dtype=\"uint8\")\n",
    "Xtest = np.array([np.array(x) for x in Xtest], dtype=\"uint8\")\n",
    "Ntest = Xtest.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional: use a smaller sample of the data\n",
    "p = np.zeros(0, dtype='int')\n",
    "for c in range(10):\n",
    "    p = np.append(p, numpy.random.choice(np.where(y==c)[0], size=100, replace=False))\n",
    "X_s = X[p,:]\n",
    "y_s = y[p]\n",
    "N_s = X_s.shape[0]\n",
    "p = np.zeros(0, dtype='int')\n",
    "for c in range(10):\n",
    "    p = np.append(p, numpy.random.choice(np.where(ytest==c)[0], size=10, replace=False))\n",
    "Xtest_s = Xtest[p,:]\n",
    "ytest_s = ytest[p]\n",
    "Ntest_s = Xtest_s.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showdigit(x):\n",
    "    \"Show one digit as a gray-scale image.\"\n",
    "    plt.imshow(x.reshape(28,28), norm=mpl.colors.Normalize(0,255), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: show first digit\n",
    "nextplot()\n",
    "showdigit(X[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showdigits(X, y, max_digits=15):\n",
    "    \"Show up to max_digits random digits per class from X with class labels from y.\"\n",
    "    num_cols = min(max_digits,  max(Counter(y).values()))\n",
    "    for c in range(10):\n",
    "        ii = np.where(y==c)[0]\n",
    "        if len(ii)>max_digits:\n",
    "            ii = numpy.random.choice(ii, size=max_digits, replace=False)\n",
    "        for j in range(num_cols):\n",
    "            ax = plt.gcf().add_subplot(10, num_cols, c*num_cols+j+1, aspect='equal')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            if j==0:\n",
    "                ax.set_ylabel(c)\n",
    "                ax.set_yticks([])\n",
    "            else:\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            if j<len(ii):\n",
    "                ax.imshow(X[ii[j],].reshape(28,28), norm=mpl.colors.Normalize(0,255), cmap='gray')\n",
    "            else:\n",
    "                ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: show 15 random digits per class from training data\n",
    "nextplot()\n",
    "showdigits(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: show a specific set of digits\n",
    "nextplot()\n",
    "showdigits(X[0:50,], y[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A simple example dataset that you can use for testing\n",
    "Xex = np.array([1,0, 0,1, 1,1, 2,0]).reshape(4,2)\n",
    "yex = np.array([0,1,2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nb_train(X, y, alpha=1, K=None, C=None):\n",
    "    \"\"\"Train a Naive Bayes model.\n",
    "\n",
    "    We assume that all features are encoded as integers and have the same domain\n",
    "    (set of possible values) from 0:(K-1). Similarly, class labels have domain\n",
    "    0:(C-1).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (N,D)\n",
    "        Design matrix.\n",
    "    y : ndarray of shape (N,)\n",
    "        Class labels.\n",
    "    alpha : int\n",
    "        Parameter for symmetric Dirichlet prior (Laplace smoothing) for all\n",
    "        fitted distributions.\n",
    "    K : int\n",
    "        Each feature takes values in [0,K-1]. None means auto-detect.\n",
    "    C : int\n",
    "        Each class label takes values in [0,C-1]. None means auto-detect.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary with the following keys and values:\n",
    "\n",
    "    logpriors : ndarray of shape (C,)\n",
    "        Log prior probabilities of each class such that logpriors[c] contains\n",
    "        the log prior probability of class c.\n",
    "\n",
    "    logcls : ndarray of shape(C,D,K)\n",
    "        A class-by-feature-by-value array of class-conditional log-likelihoods\n",
    "        such that logcls[c,j,v] contains the conditional log-likelihood of value\n",
    "        v in feature j given class c.\n",
    "    \"\"\"\n",
    "    N,D = X.shape\n",
    "    if K is None:\n",
    "        K = np.max(X)+1\n",
    "    if C is None:\n",
    "        C = np.max(y)+1\n",
    "\n",
    "    # Compute class priors and store them in priors\n",
    "    priors = np.zeros(C)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Compute class-conditional densities in a class x feature x value array\n",
    "    # and store them in cls.\n",
    "    cls = np.zeros((C,D,K))\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Output result\n",
    "    return dict(logpriors=np.log(priors), logcls=np.log(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your code (there should be a warning when you run this)\n",
    "model = nb_train(Xex, yex, alpha=1)\n",
    "model\n",
    "# This should produce:\n",
    "# {'logcls': array([[[       -inf, -0.69314718, -0.69314718],\n",
    "#          [ 0.        ,        -inf,        -inf]],\n",
    "#\n",
    "#         [[ 0.        ,        -inf,        -inf],\n",
    "#          [       -inf,  0.        ,        -inf]],\n",
    "#\n",
    "#         [[       -inf,  0.        ,        -inf],\n",
    "#          [       -inf,  0.        ,        -inf]]]),\n",
    "#  'logpriors': array([-0.69314718, -1.38629436, -1.38629436])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your code (this time no warning)\n",
    "model = nb_train(Xex, yex, alpha=2) ## here we use add-one smoothing\n",
    "model\n",
    "# This should produce:\n",
    "# {'logcls': array([[[-1.60943791, -0.91629073, -0.91629073],\n",
    "#          [-0.51082562, -1.60943791, -1.60943791]],\n",
    "#\n",
    "#         [[-0.69314718, -1.38629436, -1.38629436],\n",
    "#          [-1.38629436, -0.69314718, -1.38629436]],\n",
    "#\n",
    "#         [[-1.38629436, -0.69314718, -1.38629436],\n",
    "#          [-1.38629436, -0.69314718, -1.38629436]]]),\n",
    "#  'logpriors': array([-0.84729786, -1.25276297, -1.25276297])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    \"\"\"Computes log(sum(exp(x)).\n",
    "\n",
    "    Uses offset trick to reduce risk of numeric over- or underflow. When x is a\n",
    "    1D ndarray, computes logsumexp of its entries. When x is a 2D ndarray,\n",
    "    computes logsumexp of each row.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x : a 1D or 2D ndarray\n",
    "    \"\"\"\n",
    "    offset = np.max(x, axis=0)\n",
    "    return offset + np.log(np.sum(np.exp(x-offset), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nb_predict(model, Xnew):\n",
    "    \"\"\"Predict using a Naive Bayes model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dict\n",
    "        A Naive Bayes model trained with nb_train.\n",
    "    Xnew : nd_array of shape (Nnew,D)\n",
    "        New data to predict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary with the following keys and values:\n",
    "\n",
    "    yhat : nd_array of shape (Nnew,)\n",
    "        Predicted label for each new data point.\n",
    "\n",
    "    logprob : nd_array of shape (Nnew,)\n",
    "        Log-probability of the label predicted for each new data point.\n",
    "    \"\"\"\n",
    "    logpriors = model['logpriors']\n",
    "    logcls = model['logcls']\n",
    "    Nnew = Xnew.shape[0]\n",
    "    C,D,K = logcls.shape\n",
    "\n",
    "    # Compute the unnormalized log joint probabilities P(Y=c, x_i) of each\n",
    "    # test point (row i) and each class (column c); store in logjoint\n",
    "    logjoint = np.zeros((Nnew,C))\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Compute predicted labels (in \"yhat\") and their log probabilities\n",
    "    # P(yhat_i | x_i) (in \"logprob\")\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return dict(yhat=yhat, logprob=logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "model = nb_train(Xex, yex, alpha=2)\n",
    "nb_predict(model, Xex)\n",
    "# This should produce:\n",
    "# {'logprob': array([-0.41925843, -0.55388511, -0.68309684, -0.29804486]),\n",
    "#  'yhat': array([0, 1, 2, 0], dtype=int64)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Experiments on MNIST Digits Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's train the model on the digits data and predict\n",
    "model_nb2 = nb_train(X,y,alpha=2)\n",
    "pred_nb2 = nb_predict(model_nb2, Xtest)\n",
    "yhat = pred_nb2['yhat']\n",
    "logprob = pred_nb2['logprob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "sklearn.metrics.accuracy_score(ytest, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show some digits grouped by prediction; can you spot errors?\n",
    "nextplot()\n",
    "showdigits(Xtest, yhat)\n",
    "plt.suptitle(\"Digits grouped by predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do the same, but this time show wrong predicitions only\n",
    "perror = ytest != yhat\n",
    "nextplot()\n",
    "showdigits(Xtest[perror,:], yhat[perror])\n",
    "plt.suptitle(\"Errors grouped by predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do the same, but this time on a sample of wrong preditions to see\n",
    "# error proportions\n",
    "ierror_s = numpy.random.choice(np.where(perror)[0], 100, replace=False)\n",
    "nextplot()\n",
    "showdigits(Xtest[ierror_s,:], yhat[ierror_s])\n",
    "plt.suptitle(\"Errors grouped by predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's look at this in more detail\n",
    "print( sklearn.metrics.classification_report(ytest, yhat) )\n",
    "print( sklearn.metrics.confusion_matrix(ytest, yhat) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cumulative accuracy for predictions ordered by confidence (labels show predicted confidence)\n",
    "order = np.argsort(logprob)[::-1]\n",
    "accuracies = np.cumsum(ytest[order]==yhat[order]) / (np.arange(len(yhat)) + 1)\n",
    "nextplot()\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel('Predictions ordered by confidence')\n",
    "plt.ylabel('Accuracy')\n",
    "for x in np.linspace(0.7,1,10,endpoint=False):\n",
    "    index = int(x * (accuracies.size-1))\n",
    "    print(np.exp(logprob[order][index]))\n",
    "    plt.text(index, accuracies[index], \"{:.10f}\".format(np.exp(logprob[order][index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy for predictions grouped by confidence (labels show\n",
    "# predicted confidence). Make the plot large (or reduce number of bins) to see\n",
    "# the labels.\n",
    "bins = (np.linspace(0,1,50)*len(yhat)).astype(int)\n",
    "mean_accuracy = [ np.mean(ytest[order][bins[i]:bins[i+1]] == yhat[order][bins[i]:bins[i+1]]) for i in range(len(bins)-1) ]\n",
    "nextplot()\n",
    "plt.bar(np.arange(len(mean_accuracy)), mean_accuracy)\n",
    "plt.xticks(np.arange(len(mean_accuracy)),\n",
    "           [ \"{:.10f}\".format(x) \\\n",
    "             for x in np.exp(logprob[order][np.append(bins[1:-1], len(yhat)-1)]) ] )\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.xlabel('Confidence bin')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model Selection (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To create folds, you can use:\n",
    "K = 5\n",
    "Kf = KFold(n_splits=K, shuffle=True)\n",
    "for i_train, i_test in Kf.split(X):\n",
    "    # code here is executed K times, once per test fold\n",
    "    # i_train has the row indexes of X to be used for training\n",
    "    # i_test has the row indexes of X to be used for testing\n",
    "    print(\"Fold has {:d} training points and {:d} test points\".format(len(i_train), len(i_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use cross-validation to find a good value of alpha. Also plot the obtained\n",
    "# accuracy estimate (estimated from CV, i.e., without touching test data) as a\n",
    "# function of alpha.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nb_generate (model, ygen):\n",
    "    \"\"\"Given a Naive Bayes model, generate some data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dict\n",
    "        A Naive Bayes model trained with nb_train.\n",
    "    ygen : nd_array of shape (n,)\n",
    "        Vector of class labels for which to generate data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nd_array of shape (n,D)\n",
    "\n",
    "    Generated data. The i-th row is a sampled data point for the i-th label in\n",
    "    ygen.\n",
    "    \"\"\"\n",
    "    logcls = model['logcls']\n",
    "    n = len(ygen)\n",
    "    C,D,K = logcls.shape\n",
    "    Xgen = np.zeros((n,D))\n",
    "    for i in range(n):\n",
    "        c = ygen[i]\n",
    "        # Generate the i-th example of class c, i.e., row Xgen[i,:]. To sample\n",
    "        # from a categorical distribution with parameter theta (a probability\n",
    "        # vector), you can use numpy.random.choice(range(K),p=theta).\n",
    "        ## YOUR CODE HERE\n",
    "\n",
    "    return Xgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's generate 15 digits from each class and plot\n",
    "ygen = np.repeat(np.arange(10),15)\n",
    "Xgen = nb_generate(model_nb2, ygen)\n",
    "\n",
    "nextplot()\n",
    "showdigits(Xgen, ygen)\n",
    "plt.suptitle(\"Some generated digits for each class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can also plot the parameter vectors by choosing the most-likely\n",
    "# value for each feature\n",
    "ymax = np.arange(10)\n",
    "Xmax = np.zeros((10,D))\n",
    "for c in range(10):\n",
    "    Xmax[c,] = np.apply_along_axis(np.argmax, 1, model_nb2['logcls'][c,:,:])\n",
    "\n",
    "nextplot()\n",
    "showdigits(Xmax, ymax)\n",
    "plt.suptitle(\"Most likely value of each feature per class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or the expected value of each feature. Here we leave the categorical domain\n",
    "# and treat each feature as a number, i.e., this is NOT how categorical Naive\n",
    "# Bayes sees it and we wouldn't be able to do this if the data were really\n",
    "# categorical.\n",
    "ymean = np.arange(10)\n",
    "Xmean = np.zeros((10,D))\n",
    "for c in range(10):\n",
    "    Xmean[c,] = np.apply_along_axis(np.sum, 1, \\\n",
    "                                    np.exp(model_nb2['logcls'][c,:,:])*np.arange(256))\n",
    "\n",
    "nextplot()\n",
    "showdigits(Xmean, ymean)\n",
    "plt.suptitle(\"Expected value of each feature per class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Preprocessing Features and Continuous Naive Bayes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
