{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Hot Topics in Machine Learning, University of Mannheim, 2017\n",
    "# Author: Rainer Gemulla\n",
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "from tabulate import tabulate ## conda install tabulate\n",
    "from sortedcontainers import SortedSet ## conda install sortedcontainers\n",
    "from collections import OrderedDict\n",
    "from functools import reduce\n",
    "import numpy.random\n",
    "import codecs\n",
    "import itertools\n",
    "import pickle\n",
    "import pycrfsuite # conda install -c conda-forge pycrfsuite\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "inNotebook = False # change this to True if you use a notebook\n",
    "def nextplot():\n",
    "    if inNotebook:\n",
    "        plt.figure()  # this creates a new plot\n",
    "    else:\n",
    "        plt.clf()     # and this clears the current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After loading this file, try \"help(Var)\", \"help(Dist)\", \"help(Factor)\", and\n",
    "# \"help(FactorGraph)\". You can also have a look at this file for additional\n",
    "# details, although this shouldn't be necessary.\n",
    "%run -i \"a03-helper.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are using the Dist object to represent a joint distribution over a set of\n",
    "# variables. We usually won't create such a distribution directly, but this\n",
    "# example shows you how to do so anyway. The example is the Hayfever/Flu\n",
    "# distribution from the lecture slides.\n",
    "#\n",
    "# we first create the variables\n",
    "Hayfever = Var('Hayfever', 2) ## 2 = two possible values = binary\n",
    "Flu = Var('Flu', 2)\n",
    "print('Name =', Hayfever.name, '; domain size =', Hayfever.K, '; value =', Hayfever.value)\n",
    "print(Hayfever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and then the distribution\n",
    "dist = Dist([Hayfever, Flu], np.array([[.2, .4], [.35, .05]]))\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a distribution consists of a list of variables (vars) and an array holding the\n",
    "# joint probabilities (values)\n",
    "print([ var.name for var in dist.vars ])\n",
    "print(dist.values)\n",
    "print(dist.values[1,0]) # probability that var0=1 (Hayfever) and var1=0 (Flu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can determine the probability associated with the current values for all variables.\n",
    "print(Hayfever, Flu)\n",
    "print(dist.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can sample from a distribution; the result is another `Dist`, but this time\n",
    "# it holds sample frequencies (i.e., it's not normalized). There is a function\n",
    "# to normalize as well.\n",
    "print( dist.sample(100) )\n",
    "print( dist.sample(100).normalize() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the sum rule, variables can be marginalized out.\n",
    "print(dist.marginalize(Hayfever))\n",
    "print(dist.marginalize(Flu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, we can fix a value of a variable. Here we compute the\n",
    "# joint P(Hayfever, Flu=1), then the conditional P(Hayfever | Flu=1)\n",
    "print( dist.fix(Flu, 1) )\n",
    "print( dist.fix(Flu, 1).normalize() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create variables of misconception example\n",
    "A = Var('Anna', 2)\n",
    "B = Var('Bob', 2)\n",
    "C = Var('Charlie', 2)\n",
    "D = Var('Debbie', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create factors for misconception example. Factors are also `Dist`s, i.e., they\n",
    "# have the same functionality as illustrated above.\n",
    "phi1 = Factor([A,B])\n",
    "phi2 = Factor([B,C])\n",
    "phi3 = Factor([C,D])\n",
    "phi4 = Factor([D,A])\n",
    "phi1.values[:] = [[30,5],[1,10]]\n",
    "phi2.values[:] = [[100,1],[1,100]]\n",
    "phi3.values[:] = [[1,100],[100,1]]\n",
    "phi4.values[:] = [[100,1],[1,100]]\n",
    "print(phi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But we can also compute factor products.\n",
    "phi12 = phi1*phi2\n",
    "print(phi12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create factor graph for misconception example.\n",
    "Gm = FactorGraph()\n",
    "Gm.add_vars([A,B,C,D])\n",
    "Gm.add_factors([phi1,phi2,phi3,phi4])\n",
    "print(Gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access the data. (Use these only to read, change using the API illustrated\n",
    "# above and below.)\n",
    "print([ var.name for var in Gm.vars ])\n",
    "print( Gm.factors )\n",
    "print( Gm.factors_for[A] )\n",
    "print( Gm.value() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copy and modify factor graphs\n",
    "G = Gm.copy() # G shares variables and factors with Gm, but not the \"structure\"\n",
    "A.value = B.value = C.value = D.value = 1\n",
    "G.add_factor(phi12)\n",
    "print(G, G.value())\n",
    "G.remove_factors([phi1, phi2])\n",
    "print(G, G.value())\n",
    "G.remove_var(D) # removes associated factors, too\n",
    "print(G, G.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Inference in Factor Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive(G):\n",
    "    \"\"\"Perform naive inference on the given factor graph `G`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a ``Dist`` object for the joint distribution represented by the factor graph\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your solution; this should give (as in the lecture)\n",
    "#   Anna    Bob    Charlie    Debbie        value\n",
    "# ------  -----  ---------  --------  -----------\n",
    "#      0      0          0         0  0.041656\n",
    "#      0      0          0         1  0.041656\n",
    "#      0      0          1         0  0.041656\n",
    "#      0      0          1         1  4.1656e-06\n",
    "#      0      1          0         0  6.94267e-05\n",
    "#      0      1          0         1  6.94267e-05\n",
    "#      0      1          1         0  0.694267\n",
    "#      0      1          1         1  6.94267e-05\n",
    "#      1      0          0         0  1.38853e-05\n",
    "#      1      0          0         1  0.138853\n",
    "#      1      0          1         0  1.38853e-05\n",
    "#      1      0          1         1  1.38853e-05\n",
    "#      1      1          0         0  1.38853e-06\n",
    "#      1      1          0         1  0.0138853\n",
    "#      1      1          1         0  0.0138853\n",
    "#      1      1          1         1  0.0138853\n",
    "distm = naive(Gm)\n",
    "print(distm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To compare two distributions, we use L1 distance here. This is not necessarily\n",
    "# the most suitable measure, but it's easy to understand: sum of absolute\n",
    "# difference between corresponding probabilities.\n",
    "def l1(dist1, dist2):\n",
    "    if isinstance(dist1, Dist):\n",
    "        return np.sum(np.abs(dist1.values - dist2.values))\n",
    "    else: ## lists of marginals -> list of L1 errors\n",
    "        return np.array(list(map(lambda i: np.sum(np.abs(dist1[i]-dist2[i])),\n",
    "                                 range(len(dist1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is an example using independent sampling; we show the actual probability\n",
    "# (p), the number of samples (n), and the relative frequency (f)\n",
    "n = 10000\n",
    "samplem = distm.sample(n)\n",
    "freqm = samplem.normalize()\n",
    "print( disttable([ distm, samplem, freqm ], ['p', 'n', 'f']) )\n",
    "print( 'L1 error:', l1(distm, freqm) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b Variable elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eliminate(G, vars):\n",
    "    \"\"\"Return a copy of factor graph `G` in which `vars` have been eliminated.\"\"\"\n",
    "    Gnew = G.copy()\n",
    "    if not isinstance(vars, list):\n",
    "        vars = [ vars ]\n",
    "    for var in vars:\n",
    "        # eliminate var from Gnew using variable elimination\n",
    "        # YOUR CODE HERE\n",
    "    return Gnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test it; this should give\n",
    "#   Anna    Charlie    Debbie            p\n",
    "# ------  ---------  --------  -----------\n",
    "#      0          0         0  0.0417254\n",
    "#      0          0         1  0.0417254\n",
    "#      0          1         0  0.735923\n",
    "#      0          1         1  7.35923e-05\n",
    "#      1          0         0  1.52739e-05\n",
    "#      1          0         1  0.152739\n",
    "#      1          1         0  0.0138992\n",
    "#      1          1         1  0.0138992\n",
    "#\n",
    "#   Anna         p\n",
    "# ------  --------\n",
    "#      0  0.819448\n",
    "#      1  0.180552\n",
    "print( naive( eliminate(Gm,B) ) )\n",
    "print( naive( eliminate(Gm,[B,C,D]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this should give the same results\n",
    "print( naive(Gm).marginalize(B) )\n",
    "print( naive(Gm).marginalize([B,C,D]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gibbs(G, vars, log=False):\n",
    "    \"\"\"Resample the values of the variables in `vars` using Gibbs sampling.\n",
    "\n",
    "    Variables are resampled in the order given in `vars`. For each variable, its\n",
    "    value is resampled conditioned of the values of all other variables in `G`.\n",
    "\n",
    "    If `log` is set to `True`, resampling is performed in log space for improved\n",
    "    numerical stability.\n",
    "    \"\"\"\n",
    "    for var in vars:\n",
    "        ## Resample var. As before, you may use numpy.random.choice() to perform\n",
    "        ## the sampling.\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test it: the result of this should be close to expected. For example:\n",
    "#   Anna    expected    observed    estimated\n",
    "# ------  ----------  ----------  -----------\n",
    "#      0    0.230769         232        0.232\n",
    "#      1    0.769231         768        0.768\n",
    "B.value = C.value = 0\n",
    "D.value = 1\n",
    "expected = distm.fix(B,B.value).fix(C,C.value).fix(D,D.value).normalize()\n",
    "observed = run_gibbs(Gm, [A], 1000)\n",
    "print( disttable([expected,observed,observed.normalize()],\n",
    "                 ['expected', 'observed', 'estimated']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# similarly here; for example:\n",
    "#   Anna    Bob    Charlie    Debbie     expected    observed    estimated\n",
    "# ------  -----  ---------  --------  -----------  ----------  -----------\n",
    "#      0      0          0         0  0.041656             54        0.054\n",
    "#      0      0          0         1  0.041656             51        0.051\n",
    "#      0      0          1         0  0.041656             39        0.039\n",
    "#      0      0          1         1  4.1656e-06            0        0\n",
    "#      0      1          0         0  6.94267e-05           0        0\n",
    "#      0      1          0         1  6.94267e-05           0        0\n",
    "#      0      1          1         0  0.694267            645        0.645\n",
    "#      0      1          1         1  6.94267e-05           0        0\n",
    "#      1      0          0         0  1.38853e-05           0        0\n",
    "#      1      0          0         1  0.138853            163        0.163\n",
    "#      1      0          1         0  1.38853e-05           0        0\n",
    "#      1      0          1         1  1.38853e-05           0        0\n",
    "#      1      1          0         0  1.38853e-06           0        0\n",
    "#      1      1          0         1  0.0138853            17        0.017\n",
    "#      1      1          1         0  0.0138853            14        0.014\n",
    "#      1      1          1         1  0.0138853            17        0.017\n",
    "observed = run_gibbs(Gm, Gm.vars, 1000)\n",
    "print( disttable([distm,observed,observed.normalize()],\n",
    "                 ['expected', 'observed', 'estimated']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d Experimenting with Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From now on, we only look at marginals. Here is an example:\n",
    "def marginals(dist):\n",
    "    \"\"\"Computes the marginal probabilites of each variable in `dist`.\n",
    "\n",
    "    `dist` needs to be of type `Dist`. Returns a list with one entry per\n",
    "    variable, in the order given in `dist.vars`. Each entry is an array which\n",
    "    contains the marginal probabilities of the corresponding variable (number of\n",
    "    elements = size of domain of that variable).\n",
    "    \"\"\"\n",
    "    result = list()\n",
    "    for i in range(len(dist.vars)):\n",
    "        vars = dist.vars.copy()\n",
    "        del vars[i]\n",
    "        result.append( np.array(dist.marginalize(vars).values) )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is an example on how to use it\n",
    "n = 1000\n",
    "expected = marginals(distm)\n",
    "print(\"Expected:\", expected)\n",
    "estimated_i = marginals( distm.sample(n).normalize() )\n",
    "print(\"Estimated (independent):\", estimated_i)\n",
    "# for Gibbs sampling, use marginals argument (do NOT first compute the joint and\n",
    "# then the marignals)\n",
    "estimated_g = run_gibbs(Gm, Gm.vars, n, marginals=True, normalize=True)\n",
    "print(\"Estimated (Gibbs):\", estimated_g)\n",
    "\n",
    "## l1 on marginals gives one error entry per variable\n",
    "print(l1(expected, estimated_i))\n",
    "print(l1(expected, estimated_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now plot the average L1 estimation error with independent sampling and with Gibbs\n",
    "# sampling for various choices of n without warmup and without skips.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now fix the sample size to 1000 and investigate the impact of warmup and skip.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Factor Graphs and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's load a Naive Bayes model trained for the previous exercise\n",
    "with open('data/model_nb2.pckl', 'rb') as f:\n",
    "    model_nb2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can convert the model to a factor graph\n",
    "G_nb2, Y, Xs = nb_to_factorgraph(model_nb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to extract the image represented by the current variables (all black now\n",
    "# becaue every variable is initially 0)\n",
    "nextplot()\n",
    "showdigit( np.array([ X.value for X in Xs]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now explore how well Gibbs sampling works\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data.\n",
    "#\n",
    "# docs is a list of of documents. Each document is a list of sentences, each\n",
    "# sentence is a list of (token, label)-pairs. Label is 1 if token is part of\n",
    "# named entity, else 0.\n",
    "#\n",
    "# sentences is a list of all sentences with part-of-speech tags added. Each\n",
    "# element is a (token, part-of-speech, label)-triple.\n",
    "#\n",
    "# Dataset has been created using (if you want, see code in a03-helper.py for\n",
    "# details):\n",
    "# docs, sentences = load_reuters()\n",
    "# pickle.dump((docs,sentences), open('data/reuters.pckl', 'wb'))\n",
    "with open('data/reuters.pckl', 'rb') as f:\n",
    "    docs, sentences = pickle.load(f)\n",
    "print('Number of sentences:', len(sentences))\n",
    "print('First sentence:', sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into input, pos feature, and output sequences\n",
    "X = [ [ t for t,p,l in sentence ] for sentence in sentences ]\n",
    "Fpos_tag = [ [ p for t,p,l in sentence ] for sentence in sentences ]\n",
    "Y = [ [ str(l) for t,p,l in sentence ] for sentence in sentences ]\n",
    "print('X[0]:', X[0])\n",
    "print('Fpos_tag[0]:', Fpos_tag[0])\n",
    "print('Y[0]:', Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into training and test set (each containing sentence numbers)\n",
    "numpy.random.seed(1)\n",
    "train = list(numpy.random.choice(range(len(X)), size=int(0.8*len(X)), replace=False))\n",
    "test = [ i for i in range(len(X)) if i not in train ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify this function to experiment with other features\n",
    "def get_features(i, j):\n",
    "    \"\"\"Return a list of features for the `j`-th output of input sentence `X[i]`.\n",
    "\n",
    "    Features are binary and represented as a string. All features with the same\n",
    "    name share the same weight (parameter sharing). This function should return\n",
    "    the names of the features relevant for (i.e., connected to) the `j`-th\n",
    "    output `Y[i][j]` of input sentence `X[i]`. These features can be computed\n",
    "    from the corresponding word `X[i][j]`, but also from arbirary other elements\n",
    "    of sentence `X[i]`.\n",
    "    \"\"\"\n",
    "\n",
    "    # features for all outputs\n",
    "    features = [\n",
    "        'bias',                    # bias feature\n",
    "        'postag=' + Fpos_tag[i][j] # POS tag of current word feature\n",
    "    ]\n",
    "\n",
    "    # features for first word\n",
    "    if j==0:\n",
    "        features.extend([\n",
    "            'start'                # start of sentence bias feature\n",
    "        ])\n",
    "\n",
    "    # features for last word\n",
    "    if j==len(X[i])-1:\n",
    "        features.extend([\n",
    "            'end'                  # end of sentence bias feature\n",
    "        ])\n",
    "\n",
    "    # features for all but first word\n",
    "    if j>0:\n",
    "        features.extend([\n",
    "            '-1:postag='+Fpos_tag[i][j-1]   # POS tag of previous word feature\n",
    "        ])\n",
    "\n",
    "    # features for all but last word\n",
    "    if j<len(X[i])-1:\n",
    "        features.extend([\n",
    "            '+1:postag='+Fpos_tag[i][j+1]   # POS tag of next word feature\n",
    "        ])\n",
    "\n",
    "    # all done\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here are the feature extracted for each word of the first sentence\n",
    "for j in range(len(X[0])):\n",
    "    print(X[0][j], \":\", get_features(0,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's extract all features\n",
    "F = [ [ get_features(i,j) for j in range(len(X[i])) ] for i in range(len(X)) ]\n",
    "print(\"F[0]:\", F[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare training and testing data; we use letter F to refer to features\n",
    "def compute_train_test(F, Y, train, test):\n",
    "    Xtrain = [ X[i] for i in train]\n",
    "    Ftrain = [ F[i] for i in train ]\n",
    "    Ytrain = [ Y[i] for i in train ]\n",
    "    Xtest = [ X[i] for i in test]\n",
    "    Ftest = [ F[i] for i in test ]\n",
    "    Ytest = [ Y[i] for i in test ]\n",
    "    return Xtrain, Ftrain, Ytrain, Xtest, Ftest, Ytest\n",
    "Xtrain, Ftrain, Ytrain, Xtest, Ftest, Ytest = compute_train_test(F, Y, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use pycrfsuite to train the CRF. Fitted model is stored in file `name`.\n",
    "def crf_train(Ftrain, Ytrain, name='model.crfsuite', c1=0.1, c2=0.01, max_iterations=200):\n",
    "    trainer = pycrfsuite.Trainer(verbose=True)\n",
    "    trainer.set_params({\n",
    "        'c1': c1,                         # weight for L1 regularization\n",
    "        'c2': c2,                         # weight for L2 regularization\n",
    "        'max_iterations': max_iterations, # number of iterations for training\n",
    "        'feature.possible_transitions': True # include weights for all possible transitions\n",
    "    })\n",
    "    for f,y in zip(Ftrain, Ytrain):\n",
    "        trainer.append(f,y)\n",
    "    trainer.train(name)\n",
    "\n",
    "crf_train(Ftrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now use the model to predict\n",
    "def crf_predict(Ftest, name='model.crfsuite'):\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(name)\n",
    "    return tagger, [ tagger.tag(f) for f in Ftest ]\n",
    "\n",
    "tagger, Ypred = crf_predict(Ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a look at a a random test sentence\n",
    "i = numpy.random.choice(len(Xtest))\n",
    "for j in range(len(Xtest[i])):\n",
    "    print(Xtest[i][j], \": \", \"label=\", Ytest[i][j], \"; predicted=\", Ypred[i][j] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance of your model (micro)\n",
    "truth = np.array([ int(label) for y in Ytest for label in y ])\n",
    "pred = np.array([ int(label) for y in Ypred for label in y ])\n",
    "print( classification_report(truth, pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do everything at once\n",
    "def crf_train_and_predict(f=get_features, name='model.crfsuite',\n",
    "                      c1=0.1, c2=0.02, max_iterations=200):\n",
    "    F = [ [ f(i,j) for j in range(len(X[i])) ] for i in range(len(X)) ]\n",
    "    Xtrain, Ftrain, Ytrain, Xtest, Ftest, Ytest = compute_train_test(F, Y, train, test)\n",
    "    crf_train(Ftrain, Ytrain, name, c1, c2, max_iterations)\n",
    "    return crf_predict(Ftest, name)\n",
    "\n",
    "tagger, Ypred = crf_train_and_predict(get_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now experiment with your own set of features. Examples:\n",
    "# - the current word (e.g., in lowercase for normalization)\n",
    "# - prefix/suffix features for current word (e.g., ends with \"ion\")\n",
    "# - whether current word starts capitalized\n",
    "# - whether current word is a number\n",
    "# - whether current word contains special characters (e.g., dollar sign)\n",
    "# - similar features for surrounding words\n",
    "# - combined features of current and surrounding word\n",
    "def get_features_improved(i, j):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "tagger_improved, Ypred_improved = \\\n",
    "    crf_train_and_predict(get_features_improved, 'model_improved.crfsuite')\n",
    "pred_improved = np.array([ int(label) for y in Ypred_improved for label in y ])\n",
    "print( classification_report(truth, pred) )\n",
    "print( classification_report(truth, pred_improved) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b Feature inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inspect weights of transition features (between Y[i-1] and Y[i])\n",
    "info = tagger.info()\n",
    "transition_f = np.array([ [ l_from, l_to, info.transitions[(str(l_from),str(l_to))] ] \\\n",
    "                          for l_to in range(2) for l_from in range(2) ])\n",
    "print( tabulate(transition_f, [\"from\", \"to\", \"weight\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inspect weights of state features (for Y[i])\n",
    "state_f = np.array([ [ fl[0], fl[1], w ] \\\n",
    "                      for fl, w in info.state_features.items() ])\n",
    "print( tabulate(state_f, [\"feature\", \"label\", \"weight\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only highest weighted features\n",
    "state_f_sorted = np.array(sorted(state_f, reverse=True,\n",
    "                                 key=lambda r: abs(float(r[2]))))\n",
    "print( tabulate(state_f_sorted[:30,:], [\"feature\", \"label\", \"weight\"]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c/3d CRF to factor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert a CRF model to a factor graph (for a single test example)\n",
    "i = 1 ## CRF should make an error on this test example\n",
    "info = tagger.info()\n",
    "G = crf_to_factorgraph(Ftest[i], info)\n",
    "print(G.names())\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print true labels, predicted labels, and sampled labels\n",
    "print(\"Sentence:\")\n",
    "print(Xtest[i])\n",
    "print(\"Truth:\")\n",
    "print([ int(label) for label in Ytest[i] ])\n",
    "print(\"Predicted (MAP estimate by pycrfsuite):\")\n",
    "print([ int(label) for label in Ypred[i] ])\n",
    "nsamples = 20\n",
    "print(\"Sampled (from factor graph of CRF):\")\n",
    "for s in range(nsamples):\n",
    "    marginals = run_gibbs(G, G.vars, 1000, marginals=True)\n",
    "    print( G.values(), \"weight: {:28.0f}\".format(G.value()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert impoved CRF model to a factor graph (for a single test example)\n",
    "i = 1\n",
    "F_improved = [ [ get_features_improved(i,j)\n",
    "                 for j in range(len(X[i])) ] for i in range(len(X)) ]\n",
    "info_improved = tagger_improved.info()\n",
    "G_improved = crf_to_factorgraph(F_improved[test[i]], info_improved)\n",
    "print(G_improved.names())\n",
    "print(G_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print true labels, predicted labels, and sampled labels for improved model\n",
    "print(\"Sentence:\")\n",
    "print(Xtest[i])\n",
    "print(\"Truth:\")\n",
    "print([ int(label) for label in Ytest[i] ])\n",
    "print(\"Predicted (MAP estimate by pycrfsuite):\")\n",
    "print([ int(label) for label in Ypred_improved[i] ])\n",
    "nsamples = 20\n",
    "print(\"Sampled (from factor graph of CRF):\")\n",
    "for s in range(nsamples):\n",
    "    marginals = run_gibbs(G_improved, G_improved.vars, 1000, marginals=True)\n",
    "    print( G_improved.values(), \"weight: {:28.0f}\".format(G_improved.value()) )"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
