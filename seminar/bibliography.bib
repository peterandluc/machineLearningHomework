
@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133"
}

@inproceedings{show-reward-tell-automatic-generation-narrative-paragraph-photo-stream-adversarial-training,
author = {Wang, Jing and Fu, Jianlong and Tang, Jinhui and Li, Zechao and Mei, Tao},
title = {Show, Reward and Tell: Automatic Generation of Narrative Paragraph from Photo Stream by Adversarial Training},
booktitle = {},
year = {2018},
month = {February},
abstract = {Impressive image captioning results (i.e., an objective description for an image) are achieved with plenty of training pairs. In this paper, we take one step further to investigate the creation of narrative paragraph for a photo stream. This task is even more challenging due to the difficulty in modeling an ordered photo sequence and in generating a relevant paragraph with expressive language style for storytelling. The difficulty can even be exacerbated by the limited training data, so that existing approaches almost focus on search based solutions. To deal with these challenges, we propose a sequence-to-sequence modeling approach with reinforcement learning and adversarial training. First, to model the ordered photo stream, we propose a hierarchical recurrent neural network as story generator, which is optimized by reinforcement learning with rewards. Second, to generate relevant and story-style paragraphs, we design the rewards with two critic networks, including a multi-modal and a language style discriminator. Third, we further consider the story generator and reward critics as adversaries. The generator aims to create indistinguishable paragraphs to human-level stories,  whereas the critics aim at distinguishing them and further improving the generator by policy gradient. Experiments on three widely-used datasets show the effectiveness, against state-of-the-art methods with relative increase of 20:2% by METEOR. We also show the subjective preference for the proposed approach over the baselines through a user study with 30 human subjects.

},
publisher = {},
url = {https://www.microsoft.com/en-us/research/publication/show-reward-tell-automatic-generation-narrative-paragraph-photo-stream-adversarial-training/},
address = {},
pages = {},
journal = {},
volume = {},
chapter = {},
isbn = {},
}

@incollection{NIPS2015_5776,
title = {Expressing an Image Stream with a Sequence of Natural Sentences},
author = {Park, Cesc C and Kim, Gunhee},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {73--81},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf}
}

@Article{Simonyan14c,
    author       = "Simonyan, K. and Zisserman, A.",
    title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    journal      = "CoRR",
    volume       = "abs/1409.1556",
    year         = "2014"
}

@article{DBLP:journals/corr/MikolovSCCD13,
  author    = {Tomas Mikolov and
               Ilya Sutskever and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  journal   = {CoRR},
  volume    = {abs/1310.4546},
  year      = {2013},
  url       = {http://arxiv.org/abs/1310.4546},
  archivePrefix = {arXiv},
  eprint    = {1310.4546},
  timestamp = {Wed, 07 Jun 2017 14:40:03 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/MikolovSCCD13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@INPROCEEDINGS{1467314, 
author={S. Chopra and R. Hadsell and Y. LeCun}, 
booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
title={Learning a similarity metric discriminatively, with application to face verification}, 
year={2005}, 
volume={1}, 
number={}, 
pages={539-546 vol. 1}, 
keywords={face recognition;learning (artificial intelligence);L1 norm;discriminative loss function;face recognition;face verification;geometric distortion;semantic distance approximation;similarity metric learning;Artificial neural networks;Character generation;Drives;Face recognition;Glass;Robustness;Spatial databases;Support vector machine classification;Support vector machines;System testing}, 
doi={10.1109/CVPR.2005.202}, 
ISSN={1063-6919}, 
month={June},}

@ARTICLE{730558, 
author={L. Itti and C. Koch and E. Niebur}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={A model of saliency-based visual attention for rapid scene analysis}, 
year={1998}, 
volume={20}, 
number={11}, 
pages={1254-1259}, 
keywords={computer vision;feature extraction;image recognition;neural nets;target tracking;dynamical neural network;feature extraction;rapid scene analysis;saliency;scene understanding;target detection;topographical saliency map;visual attention;visual search;Biological system modeling;Brain modeling;Computer architecture;Feature extraction;Hardware;Image analysis;Layout;Neural networks;Object detection;Visual system}, 
doi={10.1109/34.730558}, 
ISSN={0162-8828}, 
month={Nov},}

@inproceedings{manningetal2014,
    author = {Manning, Christopher D. and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven J. and McClosky, David},
    booktitle = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
    citeulike-article-id = {13669164},
    citeulike-linkout-0 = {http://www.aclweb.org/anthology/P/P14/P14-5010},
    pages = {55--60},
    posted-at = {2015-07-10 11:40:19},
    priority = {2},
    title = {The {Stanford} {CoreNLP} Natural Language Processing Toolkit},
    url = {http://www.aclweb.org/anthology/P/P14/P14-5010},
    year = {2014}
}

@article{DBLP:journals/corr/ChungGCB14,
  author    = {Junyoung Chung and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               KyungHyun Cho and
               Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  archivePrefix = {arXiv},
  eprint    = {1412.3555},
  timestamp = {Wed, 07 Jun 2017 14:40:04 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/ChungGCB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Barzilay:2008:MLC:1350986.1350987,
 author = {Barzilay, Regina and Lapata, Mirella},
 title = {Modeling Local Coherence: An Entity-based Approach},
 journal = {Comput. Linguist.},
 issue_date = {March 2008},
 volume = {34},
 number = {1},
 month = mar,
 year = {2008},
 issn = {0891-2017},
 pages = {1--34},
 numpages = {34},
 url = {http://dx.doi.org/10.1162/coli.2008.34.1.1},
 doi = {10.1162/coli.2008.34.1.1},
 acmid = {1350987},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@InProceedings{Karpathy_2015_CVPR,
author = {Karpathy, Andrej and Fei-Fei, Li},
title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@inproceedings{Karpathy:2014:DFE:2969033.2969038,
 author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
 title = {Deep Fragment Embeddings for Bidirectional Image Sentence Mapping},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {1889--1897},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969033.2969038},
 acmid = {2969038},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@InProceedings{VQA, 
author = {Stanislaw Antol and Aishwarya Agrawal and Jiasen Lu and Margaret Mitchell and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh}, 
title = {{VQA}: {V}isual {Q}uestion {A}nswering}, 
booktitle = {International Conference on Computer Vision (ICCV)}, 
year = {2015}, 
}

@misc{vinyals2014neural,
  abstract = {Automatically describing the content of an image is a fundamental problem in
artificial intelligence that connects computer vision and natural language
processing. In this paper, we present a generative model based on a deep
recurrent architecture that combines recent advances in computer vision and
machine translation and that can be used to generate natural sentences
describing an image. The model is trained to maximize the likelihood of the
target description sentence given the training image. Experiments on several
datasets show the accuracy of the model and the fluency of the language it
learns solely from image descriptions. Our model is often quite accurate, which
we verify both qualitatively and quantitatively. For instance, while the
current state-of-the-art BLEU score (the higher the better) on the Pascal
dataset is 25, our approach yields 59, to be compared to human performance
around 69. We also show BLEU score improvements on Flickr30k, from 55 to 66,
and on SBU, from 19 to 27.},
  added-at = {2015-01-03T10:37:50.000+0100},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  biburl = {https://www.bibsonomy.org/bibtex/2045381174eb048f01434e2b551238a4c/vch},
  description = {Show and Tell: A Neural Image Caption Generator},
  interhash = {3d2777092781f0a4ceacbb47dc040dbb},
  intrahash = {045381174eb048f01434e2b551238a4c},
  keywords = {ai arxiv cs},
  note = {cite arxiv:1411.4555},
  timestamp = {2015-01-03T11:05:39.000+0100},
  title = {Show and Tell: A Neural Image Caption Generator},
  url = {http://arxiv.org/abs/1411.4555},
  year = 2014
}

@article{Karpathy:2017:DVA:3069214.3069250,
 author = {Karpathy, Andrej and Fei-Fei, Li},
 title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
 journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
 issue_date = {April 2017},
 volume = {39},
 number = {4},
 month = apr,
 year = {2017},
 issn = {0162-8828},
 pages = {664--676},
 numpages = {13},
 url = {https://doi.org/10.1109/TPAMI.2016.2598339},
 doi = {10.1109/TPAMI.2016.2598339},
 acmid = {3069250},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 
